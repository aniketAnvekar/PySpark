{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('dataaframe2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   name|age|      city|\n",
      "+-------+---+----------+\n",
      "|Rachael| 25| Baltimore|\n",
      "|    Sam| 32|    Nevada|\n",
      "| Justin| 38|New Jersey|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "\n",
    "# user_data = ['Rachael|25|Baltimore','Sam|32|Nevada','Zack|29|Sacremento','Justin|38|New Jersey']\n",
    "\n",
    "# user_data_rdd = spark.sparkContext.parallelize(user_data)\n",
    "\n",
    "\n",
    "# user_data_rdd = user_data_rdd.map(lambda ele: (ele.split('|')[0], int(ele.split('|')[1]), ele.split('|')[2]))\n",
    "\n",
    "# user_data_df = spark.createDataFrame(user_data_rdd)\n",
    "# user_data_df.show()\n",
    "\n",
    "data = [(\"Rachael\",25,\"Baltimore\"),(\"Sam\",32,\"Nevada\"),(\"Justin\",38,\"New Jersey\")]\n",
    "\n",
    "\n",
    "user_schema = StructType([\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"age\",IntegerType(),True),\n",
    "    StructField(\"city\",StringType(),True),\n",
    "])\n",
    "\n",
    "user_data_df = spark.createDataFrame(data,schema=user_schema)\n",
    "user_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Rachael', age=25, city='Baltimore')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data_df.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   name|age|     city|\n",
      "+-------+---+---------+\n",
      "|Rachael| 25|Baltimore|\n",
      "|    Sam| 32|   Nevada|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = user_data_df.filter('age<35')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data_df.createOrReplaceTempView('employee')          ### Creating view of a dataframe\n",
    "\n",
    "df2 = spark.sql(\"select * from employee where age<35\")\n",
    "\n",
    "sorted(df1.collect()) == sorted(df2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   name|age|      city|\n",
      "+-------+---+----------+\n",
      "|    Sam| 32|    Nevada|\n",
      "|   Alex| 26|      Ohio|\n",
      "| Justin| 38|New Jersey|\n",
      "|Rachael| 25| Baltimore|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a temp view and adding a new row\n",
    "\n",
    "user_data_df.createOrReplaceTempView('employee') \n",
    "df3 = spark.sql(\"\"\"create or replace temporary view employee as select * from employee union select 'Alex' as name, 26 as age, 'Ohio' as city\"\"\")\n",
    "df3 = spark.sql(\"\"\"select * from employee\"\"\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Justin', age=38, city='New Jersey'),\n",
       " Row(name='Sam', age=32, city='Nevada'),\n",
       " Row(name='Alex', age=26, city='Ohio'),\n",
       " Row(name='Rachael', age=25, city='Baltimore')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.orderBy(df3.age.desc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachael</td>\n",
       "      <td>25</td>\n",
       "      <td>Baltimore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sam</td>\n",
       "      <td>32</td>\n",
       "      <td>Nevada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Justin</td>\n",
       "      <td>38</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  age        city\n",
       "0  Rachael   25   Baltimore\n",
       "1      Sam   32      Nevada\n",
       "2   Justin   38  New Jersey"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting spark df to pandas df \n",
    "user_data_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|val|\n",
      "+---+---+\n",
      "|  1|1.5|\n",
      "|  2|3.5|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import pyarrow\n",
    "\n",
    "# df_1 = spark.createDataFrame([(1,1.0),(1,2.0),(2,3.0),(2,4.0)],('id','val'))\n",
    "\n",
    "def mean_f(key,pdf):\n",
    "    #key is a tuple of id's\n",
    "    return pd.DataFrame([key + (pdf.val.mean(),)]) \n",
    "\n",
    "df_1.groupBy(\"id\").applyInPandas(mean_f,schema='id long, val double').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-1.0.1-cp36-cp36m-win_amd64.whl (10.5 MB)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\anike\\anaconda3\\envs\\pyspark\\lib\\site-packages (from pyarrow) (1.19.1)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---+\n",
      "|    time| id| v1| v2|\n",
      "+--------+---+---+---+\n",
      "|20000101|  1|1.0|  x|\n",
      "|20000102|  1|3.0|  x|\n",
      "|20000101|  2|2.0|  y|\n",
      "|20000102|  2|4.0|  y|\n",
      "+--------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],(\"time\", \"id\", \"v1\"))\n",
    "\n",
    "df_3 = spark.createDataFrame([(20000101, 1, \"x\"), (20000101, 2, \"y\")],(\"time\", \"id\", \"v2\"))\n",
    "\n",
    "def asof_merge(d1,d2):\n",
    "    return pd.merge_asof(d1,d2,on=\"time\",by=\"id\")\n",
    "\n",
    "df_2.groupBy(\"id\").cogroup(df_3.groupBy(\"id\")).applyInPandas(asof_merge, schema=\"time int, id int, v1 double, v2 string\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
